게이트가 추가된 RNN - LSTM
=====

이전 포스트에서 RNN은 순환구조를 통해 과거의 정보를 기억할 수 있음을 알아보았다. 하지만, 사실 일반 RNN은 시간적으로 멀리 떨어진, Long Term 의존 관계를 작 학습하기 어렵다.

그래서 일반 RNN에 장기 의존 관계를 학습하기 위해 게이트(gate)라는 구조를 더한 LSTM이나 GRU 계층이 주로 사용된다.

## RNN의 문제점

그렇다면 RNN이 장기 의존 관계를 학습하기 어려운 이유는 무엇일까? 바로 BPTT 진행 시 기울기 소실 or 기울기 폭발이 일어나기 때문이다.

> - 기울기 소실 : 역전파의 기울기 값이 점점 작아지다가 아예 사라져버리는 현상 (0이 되어버려...)
> - 기울기 폭발 : 역전파의 기울기 값이 점점 커지다가 너무 과도하게 큰 값이 되어버리는 현상

![](./images/fig 6-3.png)



