순환 신경망 (Recurrent Neural Network, RNN)
====

* 피드포워드 (feed forward) 신경망 : 흐름이 단방향인 신경망

  * 구성이 단순, 구조 이해하기 쉽다. => 많은 문제에 응용 가능

  * 시계열 데이터의 성질(패턴)을 충분히 학습하기 어렵다

## 확률과 언어 모델

지금까지 살펴본 CBOW 모델의 동작,"" 맥락(t-1, t+1)이 주어졌을 때 타겟(t) 을 찾을 확률""을 구해보면 아래와 같다.

![](./images/fig 5-1.png)

![](./images/e 5-1.png)

즉, 맥락으로부터 타겟을 추측하는 것'이 목적인데, 이 목적의 실용적인 쓰임에는 어떤 것들이 있을까?

### 언어모델 (Language Model)

언어모델은 **특정한 단어의 시퀀스에 대해, 그 시퀀스가 일어날 가능성이 어느정도인지를 확률로 평가**한다. 즉, **단어의 배치/순서가 얼마나 자연스러운지** 확인하는 것이다.

* 음성 인식 시스템 : 음성 데이터로 부터 가장 자연스러운 문장을 생성하기
* 새로운 문장을 생성해내는 시스템

이를 수식으로 나타내보면 아래와 같이 동시 확률으로 표현할 수 있다:

![](./images/e 5-4.png)

즉, 동시확률은 사후확률의 총곱으로 나타낼 수 있다.

* 언어모델의 목표 : P(wt | w1, ... , wt-1) 을 구하는 것 => 동시확률 P(w1, ... , wt-1)을 구하기 위해

### CBOW 모델을 언어모델에 적용해보면?

이전에 익혔던 CBOW 모델을 억지로 언어 모델에 적용하면 어떻게 될까?

![](./images/e 5-8.png)

맥락(context)을 왼쪽 2개 단어라고 생각하면 위와 같이 근사적으로 식이 도출된다.

위에서 알 수 있듯이, CBOW를 언어모델로서 사용하게되면 맥락의 크기만큼 **고정** 되어 버린다. 즉, 맥락의 크기만큼의 단어들만 파악할 수 있고 그 크기보다 작거나 큰 위치에 있는 단어들은 아예 무시된다는 뜻이다. 이를 실제 문장에 대입하여 생각해보면 정말 CBOW는 언어모델로 사용될 수 없다는 것을 더 직접적으로 느끼게 될 것이다.

![](./images/fig 5-4.png)

위의 문장을 예로 들어보자, `?` 에 들어갈 단어를 알아내기 위해서는 첫번째 단어인 `Tom` 을 기억하고 있어야 할 것이다. 하지만 맥락이 왼쪽 방향으로 2인 경우에는 `Tom` 을 기억할 수 있을리가 없다.

그렇다면 맥락의 크기를 키우면 해결되는 걸까? 하지만 **CBOW에서는 맥락안에 속하는 단어들의 순서가 무시**된다는 특징이 있다. 애초에 CBOW 모델은 맥락 파악과 같은 시퀀셜한 데이터를 위한 모델이 아니라 단어의 의도/의미와 같이 특정 단어의 벡터에 집중하는 모델이기 때문이다.

> CBOW = continous bag-of-words (가방속의 단어 : 그냥 주머니속에 단어들이 들어있는 모습을 상상하면 된다. 즉, 순서가 없다. '순서' 대신 '분포'를 이용한다.)

순서가 무시되는 모습을 좀 더 구조적으로 보기 위해 그림을 살펴보자.

![](./images/fig 5-5.png)

CBOW 파트에서 익혔듯이, CBOW 모델은 입력들을 합하여 은닉층을 구성한다. 그렇다보니 각 입력들의 순서는 알 수 없게 되는 것이다. 그렇다면, 합하지 않고 오른쪽 그림처럼 이어붙이면 어떨까? 실제로 신경 확률론적 언어 모델(Neural Probabilistic Language Model)이 이러한 방식을 따르고 있긴하지만, 연결하는 방식으로 인해 가중치 매개변수가 맥락의 크기에 비례하여 늘어나게된다. 즉, 오버헤드가 너무 커지는 것이다.

그럼 뭘 어떻게 하라는 걸까? **맥락이 아무리 크고 길어도 그 맥락의 정보를 기억하는 메커니즘**이 필요하다!

## RNN (Recurrent Neural Network, 순환신경망)

RNN은 이름에서부터 느껴지듯이, "순환하는 신경망" 이다. 데이터가 순환되므로 과거의 정보를 기억하고 최신 데이터로 갱신할 수 있다. 즉, 정보가 계속 갱신된다.

* 순환 : 시간을 지나 다시 원래 장소로 돌아오는 과정을 반복하는 것.
  * 닫힌 경로(순환하는 경로)가 필요함

![](./images/fig 5-6.png)

위의 그림을 살펴보면, 시간 t에 대한 입력값인 Xt가 RNN계층에 입력되고 ht가 출력되고 있음을 알 수 있다. 주목해야 할 부분은 출력 시 ht값이 **분기** 되는 부분이다. 분기 될 때, ht값은 그대로 복사되어 다시 RNN 계층으로 들어간다.

RNN계층을 시간순으로 펼쳐보면 아래와 같이 표현된다.

![](./images/fig 5-8.png)

이는 마치 여러개의 계층이 순차적으로 연결되어있는 것 처럼 보인다. 하지만 이 구조는 RNN 계층 1개의 구조이다. 

위 그림을 살펴보면, 시계열 데이터인 X가 이전 시각의 출력값을 받아와 현재 시각의 출력을 계산하고 있다. 이를 수식으로 나타내면 아래와 같다.

![](./images/e 5-9.png)

* tanh 함수 (쌍곡탄젠트, hyperbolic tangent) : 활성화함수

하나의 계층에서 각 시각별로 일어나는 일로서 생각해보면 ht를 RNN 계층의 **'상태'** 로 볼 수 있다. 이 상태는 위의 규칙에 따라 갱신되는 것이다. 실제로 RNN 계층을 '상태를 가지는 계층' 또는 '메모리(기억력)가 있는 계층' 이라고도 부르곤 한다.

> RNN의 출력 h는 hidden state, hidden state vector 등으로 불린다.



### BPTT (Backpropagation Through Time)

RNN 계층은 시간 순대로 한방향으로 흘러가는 구조로 되어있기 때문에 RNN의 학습도 동일하게 거꾸로 진행하게(역전파) 된다.

![](./images/fig 5-10.png)

이렇게 **시간 방향으로 펼친 신경망의 오차역전파법**을 BPTT 라고 부른다.

그림으로만 보면 굉장히 수월하게 진행될 것 처럼 보이지만, 시계열 데이터의 t가 크면 클수록 소비되는 메모리 등의 자원이 증가하게 되기 때문에 비효율적일 수 있다.

t의 크기가 커지면 커질수록 역전파 시 기울기가 불안정해지기도 한다. 왜냐하면 블록 연결이 길어짐에 따라 기울기 값이 점점 줄어들어, 역전파가 끝나기 전에 0이 되어버려 영영 소멸해버릴 확률이 높아지기 때문이다.

### Truncated BPTT

BPTT의 단점을 보완하기 위해, 큰 시계열 데이터를 처리할 때에는 역전파의 연결을 적당한 사이즈로 끊어주는 방법이 제시되었다. 역전파의 연결만 끊어야한다.

![](./images/fig 5-11.png)

위의 그림에서도 알 수 있듯이, 10개 단위로 블록을 나눠 끊었지만 순전파의 연결은 끊지 않았다는 점에 주목하여야 한다. 즉, 데이터 입력시에 꼭 순서대로 데이터를 입력해줘야 한다.

![](./images/fig 5-14.png)

#### Truncated BPTT 의 미니배치 학습

이전의 미니배치 학습시에는 데이터를 순서대로가 아닌, 무작위로 추출하여 미니배치 학습을 진행하였다. 이번에는 순서가 꼭 지켜져야 하니 예전처럼 무작위 추출 방식은 사용할 수 없다. 

Truncated BPTT 방식으로 미니배치 학습을 할 때에는 데이터를 입력할 '시작 위치'를 각 '미니 배치의 시작 위치'로 옮겨줘야 한다.

![](./images/fig 5-15.png)

위 그림에서는 미니배치의 원소 위치(시각)에 맞는 RNN블록의 위치에 데이터를 옮겨서 입력해주고 있다.

즉, 각 미니배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 입력하면 된다! 계속 순서대로 넣어주다가 마지막 데이터에 다다르면 다시 첫번째 데이터로 돌아가서 입력해주면 된다!

### 꼭 기억해야 할 중요한 내용

* 데이털를 순서대로 제공하기!
* 미니배치별로 데이터를 제공하는 시작 블록 위치를 옮기기! (X100 데이터면 100번째 블록으로)



## RNN 구현

앞서 살펴본 RNN의 계층은 사실 **가로방향으로 성장한 신경망** 모습이라고 볼 수 있다. 그러므로 실제 구현은 가로 크기가 일정한 일련의 신경망을 구현하면 된다. 이렇게 가로방향으로 성장한 신경망을 모듈화하여 하나의 RNN계층으로 구현해야한다! 

| RNN 계층의 내부            | RNN계층                    |
| -------------------------- | -------------------------- |
| ![](./images/fig 5-16.png) | ![](./images/fig 5-17.png) |

오른쪽 그림에서와 같이 전체 신경망을 묶어서 입력 배열(xs)과 출력 배열(hs)을 하나로 출력하면 하나의 계층처럼 만들 수 있다!

위의 구조를 구현하기 위해 우리는 2가지 클래스를 구현할 것이다:

* `RNN` : RNN 계층의 내부를 구성하는 각각의 단계
* `TimeRNN` : 각 단계를 모두 모아 하나로 통합한 RNN 계층

### `RNN` 구현

 `TimeRNN` 을 구성할 `RNN` 을 먼저 구현해보자.

| RNN 순전파 수식          | 매트릭스 형상 확인         |
| ------------------------ | -------------------------- |
| ![](./images/e 5-10.png) | ![](./images/fig 5-18.png) |

일단 순전파는 위의 수식을 그대로 구현하면된다.

```python
import numpy as np

class RNN:
    def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None
    
    def forward(self, x, h_prev):
        Wx, Wh, b = self.params
        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b
        h_next = np.tanh(t)
        
        self.cache = (x, h_prev, h_next)
        return h_next
```



### `TimeRNN` 구현



## 참고자료

* [퍼플렉서티 (Perplexity) - 혼란도, 헷갈리는 정도](https://wikidocs.net/21697)

